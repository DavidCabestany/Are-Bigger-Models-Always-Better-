# Deep Learning project
### Are Large Language Models Better than Small Language Models?: A kind approach
In this work we made a comparison of the
results obtained in zero and few-shot BERT
model with the results obtained in same characteristics
DistilBERT pre-trained with Masked
Language Modeling, both making a sentimental
classification with the same dataset modified
with prompting in each instance. The expectation
is that BERT shows a better behaviour than
DistilBERT, but we aim to know how much
better is the evaluation score and make a brief
approach to both behaviours.

